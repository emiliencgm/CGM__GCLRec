diff --git a/data/yelp2018/s_pre_adj_mat.npz b/data/yelp2018/s_pre_adj_mat.npz
deleted file mode 100644
index 4d4eb20..0000000
Binary files a/data/yelp2018/s_pre_adj_mat.npz and /dev/null differ
diff --git a/pyg_code/__pycache__/augment.cpython-310.pyc b/pyg_code/__pycache__/augment.cpython-310.pyc
index e202b2f..2def07f 100644
Binary files a/pyg_code/__pycache__/augment.cpython-310.pyc and b/pyg_code/__pycache__/augment.cpython-310.pyc differ
diff --git a/pyg_code/__pycache__/dataloader.cpython-310.pyc b/pyg_code/__pycache__/dataloader.cpython-310.pyc
index 1c60fd5..a1a4ae0 100644
Binary files a/pyg_code/__pycache__/dataloader.cpython-310.pyc and b/pyg_code/__pycache__/dataloader.cpython-310.pyc differ
diff --git a/pyg_code/__pycache__/homophily.cpython-310.pyc b/pyg_code/__pycache__/homophily.cpython-310.pyc
index 2105f99..c469e1c 100644
Binary files a/pyg_code/__pycache__/homophily.cpython-310.pyc and b/pyg_code/__pycache__/homophily.cpython-310.pyc differ
diff --git a/pyg_code/__pycache__/kmeans_gpu.cpython-310.pyc b/pyg_code/__pycache__/kmeans_gpu.cpython-310.pyc
index 77af26b..7f875a8 100644
Binary files a/pyg_code/__pycache__/kmeans_gpu.cpython-310.pyc and b/pyg_code/__pycache__/kmeans_gpu.cpython-310.pyc differ
diff --git a/pyg_code/__pycache__/loss.cpython-310.pyc b/pyg_code/__pycache__/loss.cpython-310.pyc
index f9257b5..ba69684 100644
Binary files a/pyg_code/__pycache__/loss.cpython-310.pyc and b/pyg_code/__pycache__/loss.cpython-310.pyc differ
diff --git a/pyg_code/__pycache__/model.cpython-310.pyc b/pyg_code/__pycache__/model.cpython-310.pyc
index 635c50f..d39a123 100644
Binary files a/pyg_code/__pycache__/model.cpython-310.pyc and b/pyg_code/__pycache__/model.cpython-310.pyc differ
diff --git a/pyg_code/__pycache__/parse.cpython-310.pyc b/pyg_code/__pycache__/parse.cpython-310.pyc
index e3c7d51..151831c 100644
Binary files a/pyg_code/__pycache__/parse.cpython-310.pyc and b/pyg_code/__pycache__/parse.cpython-310.pyc differ
diff --git a/pyg_code/__pycache__/precalcul.cpython-310.pyc b/pyg_code/__pycache__/precalcul.cpython-310.pyc
index 31307ca..b873606 100644
Binary files a/pyg_code/__pycache__/precalcul.cpython-310.pyc and b/pyg_code/__pycache__/precalcul.cpython-310.pyc differ
diff --git a/pyg_code/__pycache__/procedure.cpython-310.pyc b/pyg_code/__pycache__/procedure.cpython-310.pyc
index 243bf99..0a867b2 100644
Binary files a/pyg_code/__pycache__/procedure.cpython-310.pyc and b/pyg_code/__pycache__/procedure.cpython-310.pyc differ
diff --git a/pyg_code/__pycache__/utils.cpython-310.pyc b/pyg_code/__pycache__/utils.cpython-310.pyc
index 40b27a3..18ac6de 100644
Binary files a/pyg_code/__pycache__/utils.cpython-310.pyc and b/pyg_code/__pycache__/utils.cpython-310.pyc differ
diff --git a/pyg_code/__pycache__/visual.cpython-310.pyc b/pyg_code/__pycache__/visual.cpython-310.pyc
index 7403999..19a022c 100644
Binary files a/pyg_code/__pycache__/visual.cpython-310.pyc and b/pyg_code/__pycache__/visual.cpython-310.pyc differ
diff --git a/pyg_code/__pycache__/world.cpython-310.pyc b/pyg_code/__pycache__/world.cpython-310.pyc
index 12fe0c4..9034de1 100644
Binary files a/pyg_code/__pycache__/world.cpython-310.pyc and b/pyg_code/__pycache__/world.cpython-310.pyc differ
diff --git a/pyg_code/dataloader.py b/pyg_code/dataloader.py
index 31e30e0..db33bf9 100644
--- a/pyg_code/dataloader.py
+++ b/pyg_code/dataloader.py
@@ -21,7 +21,7 @@ from time import time
 import networkx as nx
 from torch_geometric.data import Data
 
-class dataset(Dataset):
+class dataset():
     """
     Dataset type for pytorch \n
     Incldue graph information
@@ -355,24 +355,3 @@ class dataset(Dataset):
         edge_indice = torch.sparse.FloatTensor(index, val, (self.n_user, self.m_item))
         edge_indice = edge_indice.coalesce()
         return edge_indice
-
-    '''
-    Dataset的所有子类都应该重写方法__len__(), __getitem__()
-    '''
-    def __len__(self):
-        return self.traindataSize
-
-    def __getitem__(self, idx):
-        '''
-        input: user在trainUser列表中的idx
-        output: 随机三元组(user, pos, neg)
-        '''
-        user = self.trainUser[idx]
-        pos = random.choice(self._allPos[user])
-        while True:
-            neg = np.random.randint(0, self.m_item)
-            if neg in self._allPos[user]:
-                continue
-            else:
-                break
-        return user, pos, neg
diff --git a/pyg_code/main.py b/pyg_code/main.py
index 495e796..252edb7 100644
--- a/pyg_code/main.py
+++ b/pyg_code/main.py
@@ -103,7 +103,7 @@ def main():
     notes = world.config['notes']
     group = world.config['group']
     job_type = world.config['job_type']
-    os.environ['WANDB_MODE'] = 'dryrun'#TODO WandB上传
+    # os.environ['WANDB_MODE'] = 'dryrun'#TODO WandB上传
     wandb.init(project=project, name=name, tags=tag, group=group, job_type=job_type, config=world.config, save_code=True, sync_tensorboard=False, notes=notes)
     wandb.define_metric("custom_epoch")
     wandb.define_metric(f"{world.config['dataset']}"+'/loss', step_metric='custom_epoch')
@@ -137,6 +137,11 @@ def main():
     print('precal cost : ',end-start)
     cprint('[PRECALCULATE--END]')
 
+    cprint('[SAMPLER--START]')
+    sampler = precalcul.sampler(dataset=dataset, precal=precal)
+    cprint('[SAMPLER--END]')
+    
+
     models = {'LightGCN':model.LightGCN}
     Recmodel = models[world.config['model']](world.config, dataset, precal).to(world.device)
 
@@ -185,11 +190,15 @@ def main():
     #     optimizer.add_param_group({'params':augmentation.mlp_edge_model.parameters()})
 
     emb_optimizer = torch.optim.Adam(Recmodel.parameters(), lr=world.config['lr'])
-    aug_optimizer = torch.optim.Adam(augmentation.parameters(), lr=world.config['lr'])    
+    if world.config['augment'] in ['Learner']:
+        aug_optimizer = torch.optim.Adam(augmentation.parameters(), lr=world.config['lr'])    
     emb_optimizer.add_param_group({'params':total_loss.MLP_model.parameters()})#TODO Adaloss 在哪一步更新
     # aug_optimizer = torch.optim.Adam([{'params':augmentation.GNN_encoder.parameters()}, 
     #                                 {'params':augmentation.mlp_edge_model.parameters()}], lr=world.config['lr'])
-    optimizer = {'emb':emb_optimizer, 'aug':aug_optimizer}
+    if world.config['augment'] in ['Learner']:
+        optimizer = {'emb':emb_optimizer, 'aug':aug_optimizer}
+    else:
+        optimizer = {'emb':emb_optimizer}
     quantify = visual.Quantify(dataset, Recmodel, precal)
 
 
@@ -223,7 +232,7 @@ def main():
                     
             cprint('[TRAIN]')
             start_train = time.time()
-            avg_loss = train.train(dataset, Recmodel, augmentation, epoch, optimizer)
+            avg_loss = train.train(sampler, Recmodel, augmentation, epoch, optimizer)
             end_train = time.time()
             wandb.log({ f"{world.config['dataset']}"+'/loss': avg_loss})
             wandb.log({f"{world.config['dataset']}"+f"/training_time": end_train - start_train})
diff --git a/pyg_code/parse.py b/pyg_code/parse.py
index 491a6c1..22fd4bc 100644
--- a/pyg_code/parse.py
+++ b/pyg_code/parse.py
@@ -46,7 +46,7 @@ def parse_args():
     parser.add_argument('--dataset', type=str, default='yelp2018', help="dataset:[yelp2018,  gawalla, ifashion, amazon-book, last-fm, MIND]") 
     parser.add_argument('--seed', type=int, default=2023, help="random seed")
     parser.add_argument('--loss', type=str, default='Adaptive', help="loss function: Adaptive")
-    parser.add_argument('--augment', type=str, default='Learner', help="Augmentation: No, Adaptive, Learner")    
+    parser.add_argument('--augment', type=str, default='No', help="Augmentation: No, Adaptive, Learner")    
     parser.add_argument('--centroid_mode', type=str, default='eigenvector', help="Centroid mode: degree, pagerank, eigenvector")
     parser.add_argument('--commonNeighbor_mode', type=str, default='SC', help="Common Neighbor mode: JS, SC, CN, LHN")
     parser.add_argument('--adaptive_method', type=str, default='mlp', help="Adaptive coef method: centroid, commonNeighbor, homophily, mlp")
@@ -58,6 +58,7 @@ def parse_args():
     parser.add_argument('--adaloss_mode', type=str, default='pos', help="mode of AdaLoss: pos, pos+neg, pos+neg+cl")
     parser.add_argument('--if_adaptive', type=int, default=1, help="=1: use adaptive coef. =0: use +1.")
     parser.add_argument('--freeze_mlp', type=int, default=500, help="freeze MLP parameters after n epochs")
+    parser.add_argument('--sampling', type=str, default='uij', help="sampling method")
     #===========================================================================================================================================
     parser.add_argument('--c', type=str, default='testing', help="note something for this experiment")
 
diff --git a/pyg_code/precalcul.py b/pyg_code/precalcul.py
index a4f14a5..74ce8cd 100644
--- a/pyg_code/precalcul.py
+++ b/pyg_code/precalcul.py
@@ -16,6 +16,8 @@ import os
 import time
 from scipy.sparse import csr_matrix
 import torch_sparse
+from torch.utils.data import Dataset
+import random
 
 #=============================================================Overall Precalculate============================================================#
 class precalculate():
@@ -663,4 +665,57 @@ class SVD():
         val = torch.ones(graph.values().shape[0]).to(world.device)
         num_nodes = graph.shape[0]
         adj = torch.sparse.FloatTensor(graph.indices(), val, torch.Size([num_nodes, num_nodes]))
-        return adj
\ No newline at end of file
+        return adj
+    
+
+class sampler(Dataset):
+    def __init__(self, dataset, precal):
+        self.traindataSize = dataset.traindataSize
+        self.trainUser = dataset.trainUser
+        self.m_item = dataset.m_item
+        self._allPos = dataset._allPos
+        self.reverse_ItemPopGroupDict = precal.popularity.reverse_ItemPopGroupDict
+        self.ItemPopGroupDict = precal.popularity.ItemPopGroupDict
+
+    def __len__(self):
+        return self.traindataSize
+
+    def __getitem__(self, idx):
+        '''
+        input: user在trainUser列表中的idx
+        output: 随机三元组(user, pos, neg) or (user, pos, pos', neg)
+        pos'的popgroup和pos不同
+        '''
+        if world.config['sampling'] == 'uij':
+            user = self.trainUser[idx]
+            pos = random.choice(self._allPos[user])
+            while True:
+                neg = np.random.randint(0, self.m_item)
+                if neg in self._allPos[user]:
+                    continue
+                else:
+                    break
+            return user, pos, neg
+        
+        elif world.config['sampling'] == 'uiij':
+            user = self.trainUser[idx]
+            pos1 = random.choice(self._allPos[user])
+            group1 = self.reverse_ItemPopGroupDict[pos1]
+            
+            for i in range(20):#若20次采样都没有获得不同pop分组的另一个正样本则随机采样pos2
+                pos2 = random.choice(self._allPos[user])
+                if pos2 in self.ItemPopGroupDict[group1]:
+                    continue
+                else:
+                    break
+
+            while True:
+                neg = np.random.randint(0, self.m_item)
+                if neg in self._allPos[user]:
+                    continue
+                else:
+                    break
+            return user, pos1, pos2, neg
+        
+        else:
+            raise(TypeError)    
\ No newline at end of file
diff --git a/pyg_code/procedure.py b/pyg_code/procedure.py
index 32097d1..2d1a3a0 100644
--- a/pyg_code/procedure.py
+++ b/pyg_code/procedure.py
@@ -25,26 +25,64 @@ class Train():
         self.INFONCE = loss.InfoNCE_loss()
         self.BPR = loss.BPR()
 
-    def train(self, dataset, Recmodel, augmentation, epoch, optimizer):
+    def train(self, sampler, Recmodel, augmentation, epoch, optimizer):
         Recmodel:LightGCN = Recmodel
         batch_size = world.config['batch_size']
-        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0)#每个batch为batch_size对(user, pos_item, neg_item), 见Dataset.__getitem__
+        dataloader = DataLoader(sampler, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0)#每个batch为batch_size对(user, pos_item, neg_item), 见Dataset.__getitem__
 
         total_batch = len(dataloader)
         aver_loss = 0.
 
         for batch_i, train_data in tqdm(enumerate(dataloader), desc='training'):
-            batch_users = train_data[0].long().to(world.device)
-            batch_pos = train_data[1].long().to(world.device)
-            batch_neg = train_data[2].long().to(world.device)
+            if world.config['sampling'] == 'uij':
+                batch_users = train_data[0].long().to(world.device)
+                batch_pos = train_data[1].long().to(world.device)
+                batch_neg = train_data[2].long().to(world.device)
+            elif world.config['sampling'] == 'uiij':
+                batch_users = train_data[0].long().to(world.device)
+                batch_pos1 = train_data[1].long().to(world.device)
+                batch_pos2 = train_data[2].long().to(world.device)
+                batch_neg = train_data[3].long().to(world.device)
+            else:
+                pass
+            # batch_users = train_data[0].long().to(world.device)
+            # batch_pos = train_data[1].long().to(world.device)
+            # batch_neg = train_data[2].long().to(world.device)
             
-            l_all = self.train_all(Recmodel, augmentation, batch_users, batch_pos, batch_neg, optimizer, epoch)
+            l_all = self.train_now(Recmodel, augmentation, batch_users, batch_pos1, batch_pos2, batch_neg, optimizer, epoch)
 
             aver_loss += l_all.cpu().item()
         aver_loss = aver_loss / (total_batch)
         print(f'EPOCH[{epoch}]:loss {aver_loss:.3f}')
         return aver_loss
-    
+
+    def train_now(self, Recmodel, augmentation, batch_users, batch_pos1, batch_pos2, batch_neg, optimizer, epoch):
+        all_users, all_items = Recmodel.computer()
+        users_emb = all_users[batch_users]
+        pos_emb1 = all_items[batch_pos1]
+        pos_emb2 = all_items[batch_pos2]
+        neg_emb = all_items[batch_neg]
+        users_emb_ego = Recmodel.embedding_user(batch_users)
+        pos_emb_ego1 = Recmodel.embedding_item(batch_pos1)
+        pos_emb_ego2 = Recmodel.embedding_item(batch_pos2)
+        neg_emb_ego = Recmodel.embedding_item(batch_neg)
+
+        pos_aug = self.mixup(pos_emb1, pos_emb2)
+
+        reg = (1/8)*(users_emb_ego.norm(2).pow(2) + pos_emb_ego1.norm(2).pow(2) + pos_emb_ego2.norm(2).pow(2) + neg_emb_ego.norm(2).pow(2))/len(batch_users)
+
+        cl = self.INFONCE.cal_infonce_loss(users_emb, pos_emb1) + self.INFONCE.cal_infonce_loss(users_emb, pos_emb2) + self.INFONCE.cal_infonce_loss(users_emb, pos_aug)
+        cl = cl*(1/3)
+
+        loss = cl + world.config['weight_decay']*reg
+
+        optimizer['emb'].zero_grad()
+        loss.backward()
+        optimizer['emb'].step()
+
+        return loss
+
+
     def train_batch(self, Recmodel, augmentation, batch_users, batch_pos, batch_neg, optimizer, epoch):
         #========================train Augmentation==========================
         #计算增强视图下的表示
@@ -289,6 +327,14 @@ class Train():
 
         return loss
     
+    def mixup(self, x1, x2):
+        alpha = 2.
+        beta = 2.
+        size = [len(x1), 1]
+        l = np.random.beta(alpha, beta, size)
+        mixed_x = torch.tensor(l, dtype=torch.float32).to(x1.device) * x1 + torch.tensor(1-l, dtype=torch.float32).to(x2.device) * x2
+        return mixed_x
+    
 class Test():
     def __init__(self):
         pass
diff --git a/pyg_code/world.py b/pyg_code/world.py
index 4310993..1cfefb0 100644
--- a/pyg_code/world.py
+++ b/pyg_code/world.py
@@ -59,6 +59,7 @@ config['if_valid'] = args.if_valid
 config['adaloss_mode'] = args.adaloss_mode
 config['if_adaptive'] = args.if_adaptive
 config['freeze_mlp'] = args.freeze_mlp
+config['sampling'] = args.sampling
 config['comment'] = args.comment
 #WandB
 config['project'] = args.project
